# 爬虫基本原理

爬虫是 模拟用户在浏览器或者App应用上的操作，把操作的过程、实现自动化的程序

当我们在浏览器中输入一个url后回车，后台会发生什么？比如说你输入`https://www.baidu.com`

简单来说这段过程发生了以下四个步骤：

- 查找域名对应的IP地址。

  浏览器首先访问的是DNS(Domain Name System,域名系统),dns的主要工作就是把域名转换成相应的IP地址

- 向IP对应的服务器发送请求。

- 服务器响应请求，发回网页内容。

- 浏览器显示网页内容。

  ![浏览器工作原理](imgs/spider/webdns.jpg)
  
  网络爬虫要做的，简单来说，就是实现浏览器的功能。通过指定url，直接返回给用户所需要的数据， 而不需要一步步人工去操纵浏览器获取。

# 爬取模板的使用

## urllib

urllib提供了一系列用于操作URL的功能。

### urlretrieve

urlretrieve方法将url定位到的html文件下载到本地的硬盘中。

如果不指定filename，则会存为临时文件。

urlretrieve()返回一个二元组(filename,mine_hdrs)

```python
from urllib import request

path, headers = request.urlretrieve("https://www.baidu.com", "tmp.html")
print(path, dict(headers))
```

返回：

```
tmp.html {'Accept-Ranges': 'bytes', 'Cache-Control': 'no-cache', 'Content-Length': '227', 'Content-Type': 'text/html', 'Date': 'Fri, 06 Dec 2019 16:35:05 GMT', 'P3p': 'CP=" OTI DSP COR IVA OUR IND COM "', 'Pragma': 'no-cache', 'Server': 'BWS/1.1', 'Set-Cookie': 'BD_NOT_HTTPS=1; path=/; Max-Age=300', 'Strict-Transport-Security': 'max-age=0', 'Traceid': '1575650105268192231414425532091877651297', 'X-Ua-Compatible': 'IE=Edge,chrome=1', 'Connection': 'close'}
```

### urlencode

```python
urlencode = parse.urlencode({"t": "b", "w": "python"})
print(urlencode)
# t=b&w=python
```



### Get

urllib的request模块可以非常方便地抓取URL内容，也就是发送一个GET请求到指定的页面，然后返回HTTP的响应：

```python
from urllib import request
with request.urlopen('https://suggest.taobao.com/sug?code=utf-8&q=python') as f:
    data = f.read()
    print('Status:', f.status, f.reason)
    for k, v in f.getheaders():
        print('%s: %s' % (k, v))
    print('Data:', data.decode('utf-8'))
```

可以看到HTTP响应的头和JSON数据：

```
Status: 200 OK
Date: Fri, 06 Dec 2019 16:24:32 GMT
Content-Type: text/html; charset=utf-8
Transfer-Encoding: chunked
Connection: close
Vary: Accept-Encoding
multicall_ec: 0
Server: Tengine/Aserver
Strict-Transport-Security: max-age=31536000
Timing-Allow-Origin: *
EagleEye-TraceId: 0ab94ab015756494720526350e0ac1
Data: 
{"result":[["python编程从入门","1023.0098187311178"],["python基础教程","1430.4520396912899"],["python数据分析基础","558.5761316872428"],["python开发板","289.5"],["python代做 学生","511.90453460620523"],["python二级","250.2462311557789"],["python编程从入门到实战","278.4130434782609"],["python学习手册","335.02702702702703"],["python语言程序设计","1196.659574468085"],["python程序设计基础","754.7972972972973"]]}
```

 如果我们要想模拟浏览器发送GET请求，就需要使用Request对象，通过往Request对象添加HTTP头，我们就可以把请求伪装成浏览器。例如，模拟iPhone 6去请求豆瓣首页：

```python
from urllib import request

req = request.Request('http://www.douban.com/')
req.add_header('User-Agent', 'Mozilla/6.0 (iPhone; CPU iPhone OS 8_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/8.0 Mobile/10A5376e Safari/8536.25')
with request.urlopen(req) as f:
    print('Status:', f.status, f.reason)
    for k, v in f.getheaders():
        print('%s: %s' % (k, v))
    print('Data:', f.read().decode('utf-8'))
```

这样豆瓣会返回适合iPhone的移动版网页.

### Post

如果要以POST发送一个请求，只需要把参数data以bytes形式传入。

模拟一个微博登录，先读取登录的邮箱和口令，然后按照weibo.cn的登录页的格式以username=xxx&password=xxx的编码传入：

```python
from urllib import request, parse

print('Login to post page...')
email = input('Email: ')
passwd = input('Password: ')
login_data = parse.urlencode([
    ('username', email),
    ('password', passwd)
])

req = request.Request(' http://127.0.0.1:5000/signin')
req.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36')
req.add_header('Referer', 'https://passport.weibo.cn/signin/login?entry=mweibo&res=wel&wm=3349&r=http%3A%2F%2Fm.weibo.cn%2F')
with request.urlopen(req, data=login_data.encode('utf-8')) as f:
    print('Status:', f.status, f.reason)
    for k, v in f.getheaders():
        print('%s: %s' % (k, v))
    print('Data:', f.read().decode('utf-8'))
```

### Handler

如果还需要更复杂的控制，比如通过一个Proxy去访问网站，我们需要利用ProxyHandler来处理，示例代码如下：

```python
import urllib

proxy_handler = urllib.request.ProxyHandler({'http': 'http://www.example.com:3128/'})
proxy_auth_handler = urllib.request.ProxyBasicAuthHandler()
proxy_auth_handler.add_password('realm', 'host', 'username', 'password')
opener = urllib.request.build_opener(proxy_handler, proxy_auth_handler)
with opener.open('http://www.example.com/login.html') as f:
    pass
```

 

## requests

Python内置的urllib模块用于访问网络资源。但是，它用起来比较麻烦，而且，缺少很多实用的高级功能。

更好的方案是使用requests。它是一个Python第三方库，处理URL资源特别方便。

 如果安装了Anaconda，requests就已经可用了。否则，需要在命令行下通过pip安装：

```
pip install requests
```

### GET访问

```python
import requests
r = requests.get('https://www.douban.com/') # 豆瓣首页

print(r.text)
```

传入一个dict作为params参数：

```python
r = requests.get('https://www.douban.com/search', params={'q': 'python', 'cat': '1001'})
print(r.url) # 实际请求的URL
# https://www.douban.com/search?q=python&cat=1001
```

requests自动检测编码，可以使用encoding属性查看：

```python
print(r.encoding)
# utf-8
```

用content属性获得bytes二进制内容：

```python
print(r.content)
```

对于特定类型的响应，例如JSON，可以直接获取：

```python
r = requests.get("https://www.tianqiapi.com/api/?version=v1&city=深圳&appid=13639281&appsecret=yCJK9RVk")
print(r.json())
```

传入一个dict作为headers参数：

```python
r = requests.get('https://www.douban.com/', headers={'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit'})
print(r.text)
```

### POST

要发送POST请求，只需要把get()方法变成post()，然后传入data参数作为POST请求的数据：

```python
r = requests.post('https://accounts.douban.com/login', data={'form_email': 'abc@example.com', 'form_password': '123456'})
```

requests默认使用application/x-www-form-urlencoded对POST数据编码。如果要传递JSON数据，可以直接传入json参数：

```python
params = {'key': 'value'}
r = requests.post(url, json=params) # 内部自动序列化为JSON
```

类似的，上传文件需要更复杂的编码格式，但是requests把它简化成files参数：

```python
upload_files = {'file': open('report.xls', 'rb')}
r = requests.post(url, files=upload_files)
```

 在读取文件时，注意务必使用`'rb'`即二进制模式读取，这样获取的`bytes`长度才是文件的长度。 

把post()方法替换为put()，delete()等，就可以以PUT或DELETE方式请求资源。

除了能轻松获取响应内容外，requests对获取HTTP响应的其他信息也非常简单。例如，获取响应头：

```python
r.headers
r.headers['Content-Type']

# 获取指定的Cookie：
r.cookies['ts']

# 传入cookies参数：
cs = {'token': '12345', 'status': 'working'}
r = requests.get(url, cookies=cs)

# 指定timeout超时参数：
r = requests.get(url, timeout=2.5) # 2.5秒后超时
```

#  解析模板的使用

## Beautiful Soup

### 简介

简单来说，Beautiful Soup是python的一个库，最主要的功能是从网页抓取数据。官方解释如下：

> Beautiful Soup提供一些简单的、python式的函数用来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。 Beautiful Soup自动将输入文档转换为Unicode编码，输出文档转换为utf-8编码。你不需要考虑编码方式，除非文档没有指定一个编码方式，这时，Beautiful Soup就不能自动识别编码方式了。然后，你仅仅需要说明一下原始编码方式就可以了。 Beautiful Soup已成为和lxml、html6lib一样出色的python解释器，为用户灵活地提供不同的解析策略或强劲的速度。

```python
from bs4 import BeautifulSoup

html = """<html><head><title>The Dormouse's story</title></head>
<body>
<p class="title" name="dromouse"><b>The Dormouse's story</b></p>
<p class="story">Once upon a time there were three little sisters; and their names were
<a href="http://example.com/elsie" class="sister" id="link1"><!-- Elsie --></a>,
<a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
<a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
and they lived at the bottom of a well.</p>
<p class="story">...</p>
"""
soup = BeautifulSoup(html)
print(soup.prettify())
```

### 找标签

```python
print(soup.title)
# <title>The Dormouse's story</title>
print(soup.head)
# <head><title>The Dormouse's story</title></head>
print(soup.a)
# <a class="sister" href="http://example.com/elsie" id="link1"><!-- Elsie --></a>
print(soup.p)
# <p class="title" name="dromouse"><b>The Dormouse's story</b></p>
```

对于标签，它有两个重要的属性，是 name 和 attrs：

```python
print(soup.name)
# [document]
print(soup.head.name)
# head
```

soup 对象本身比较特殊，它的 name 即为 [document]，对于其他内部标签，输出的值便为标签本身的名称

```python
print(soup.p.attrs)
#{'class': ['title'], 'name': 'dromouse'}
```

在这里，我们把 p 标签的所有属性打印输出了出来，得到的类型是一个字典。

如果想要单独获取某个属性：

```python
print(soup.p['class'])
# ['title']
```

获取文本：

```python
print(soup.p.string)
# The Dormouse's story
```

支持css选择器，方法是调用select选择器：

```python
print(soup.select('a[href="http://example.com/elsie"]'))
# [<a class="sister" href="http://example.com/elsie" id="link1"><!-- Elsie --></a>]
```

